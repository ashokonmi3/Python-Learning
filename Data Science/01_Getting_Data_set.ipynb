{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a Dataset for Machine Learning: \n",
    "In machine learning, the **dataset** is the collection of information or data that the computer uses to learn from. The model needs this data to make predictions or decisions. Let’s break it down step by step to make it easier to understand:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **What is a Dataset?**\n",
    "\n",
    "A **dataset** is like a collection of **examples** or **records**. Each example has different **features** (pieces of information), and together, these examples help the model learn.\n",
    "\n",
    "- **Real-Life Example**: Think of a **dataset** as a **recipe book**. Each recipe in the book contains ingredients (features) and instructions (outcomes). If you want to make a cake, the **ingredients** are the data, and the **cake recipe** is what the model learns from.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Where Do You Get a Dataset?**\n",
    "\n",
    "Datasets can come from many places:\n",
    "\n",
    "- **Public Datasets**: These are datasets that anyone can access, usually available online. Some websites, like Kaggle or UCI Machine Learning Repository, provide free datasets.\n",
    "  \n",
    "  - **Real-Life Example**: Just like finding free recipes online, you can download free datasets online. For example, a dataset might contain the heights and weights of different people, and you can use this data to predict someone’s weight based on their height.\n",
    "\n",
    "- **Creating Your Own Dataset**: If you don’t find the right dataset, you can create one. This could be collecting information from surveys, questionnaires, or your own observations.\n",
    "\n",
    "  - **Real-Life Example**: If you run a lemonade stand, you could collect data on the number of cups sold each day and the weather. You could then use this data to predict how many cups you might sell on a hot day.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Types of Data in a Dataset**\n",
    "\n",
    "- **Structured Data**: This is data that is organized in rows and columns, like a spreadsheet or table. It's easy to read and analyze.\n",
    "  \n",
    "  - **Real-Life Example**: A table that lists people’s names, ages, and favorite fruits would be **structured data**. Each row represents a person, and each column has a different type of information (name, age, favorite fruit).\n",
    "\n",
    "- **Unstructured Data**: This is data that doesn’t follow a specific format, like text, images, or videos. It needs extra work to be turned into something the model can learn from.\n",
    "  \n",
    "  - **Real-Life Example**: If you take pictures of fruits, that would be **unstructured data**. You need special tools or techniques to understand the content of those pictures.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Cleaning the Dataset**\n",
    "\n",
    "Before using the data, it often needs to be cleaned. This means getting rid of bad or missing data, fixing mistakes, and organizing it in a way the model can understand.\n",
    "\n",
    "- **Real-Life Example**: Imagine you’re using a recipe book, but one recipe has the wrong ingredient or the instructions are incomplete. You would fix those mistakes before trying to cook. In machine learning, we fix mistakes in the dataset before using it.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Labeling the Dataset**\n",
    "\n",
    "In some cases, the dataset will have **labels** (the correct answers) that the model will learn to predict. If you want the model to predict something, you need to show the model the correct answers during training.\n",
    "\n",
    "- **Real-Life Example**: If you want to teach a model to recognize apples, you might show it pictures of apples with labels that say “apple.” The model learns that the label “apple” goes with a certain type of fruit.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Using the Dataset to Train the Model**\n",
    "\n",
    "Once you have the dataset, you use it to train the model. Training is like teaching the computer. The more examples you give it, the better it can learn.\n",
    "\n",
    "- **Real-Life Example**: If you’re learning to bake cookies, you follow the recipe repeatedly. The more you practice, the better you get at baking cookies. Similarly, the model improves its predictions the more data it is trained with.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Summary**\n",
    "\n",
    "- **Dataset**: A collection of data that the model uses to learn.\n",
    "- **Where to get it**: Public datasets online or create your own.\n",
    "- **Types of data**: Structured (like tables) and unstructured (like images or text).\n",
    "- **Cleaning**: Fixing mistakes and organizing data before using it.\n",
    "- **Labeling**: Showing the model the correct answers during training.\n",
    "- **Training**: Teaching the model using the data.\n",
    "\n",
    "---\n",
    "\n",
    "By understanding where datasets come from and how to work with them, you’ll be better equipped to start using machine learning. Remember, a good dataset is key to building a successful machine learning model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "# load_iris: This function from sklearn.datasets loads the famous Iris dataset, \n",
    "# which contains information about different species of iris flowers, such as sepal length, \n",
    "# sepal width, petal length, and petal width.\n",
    "\n",
    "# The dataset contains:\n",
    "# iris.data: A numpy array with the feature data (sepal length, sepal width, petal length, petal width).\n",
    "# iris.target: A numpy array with the target labels, which correspond to the species of the iris flowers (setosa, versicolor, or virginica).\n",
    "# iris.feature_names: A list of the names of the features (sepal length, sepal width, petal length, petal width).\n",
    "\n",
    "# Convert to a DataFrame for easier viewing\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Display the top 5 rows of the dataset\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset from the Hugging Face datasets library\n",
    "ds = load_dataset(\"scikit-learn/iris\")\n",
    "\n",
    "# Convert the 'train' split to a Pandas DataFrame\n",
    "iris_df = pd.DataFrame(ds['train'])\n",
    "# Datasets are split into 'train', 'validation', and 'test' for machine learning purposes,\n",
    "# and the 'train' split contains the data used to train models.\n",
    "# the key \"train\" is predefined and used to store the entire dataset. I\n",
    "\n",
    "# Display the first 5 rows of the dataset in a table format\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Titanic dataset using seaborn\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(titanic_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "\n",
    "# Example: Load the Titanic dataset from Kaggle\n",
    "kaggle.api.dataset_download_files('heptapod/titanic', path='titanic_data', unzip=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset into a pandas DataFrame\n",
    "titanic_df = pd.read_csv('titanic_data/train.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(titanic_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country', 'Age', 'Salary', 'Purchased']\n",
      "['France', '44', '72000', 'No']\n",
      "['Spain', '27', '48000', 'Yes']\n",
      "['Germany', '30', '54000', 'No']\n",
      "['Spain', '38', '61000', 'No']\n",
      "['Germany', '40', '', 'Yes']\n",
      "['France', '35', '58000', 'Yes']\n",
      "['Spain', '', '52000', 'No']\n",
      "['France', '48', '79000', 'Yes']\n",
      "['Germany', '50', '83000', 'No']\n",
      "['France', '37', '67000', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('Data.csv', 'r') as file:\n",
    "   reader = csv.reader(file)\n",
    "   for row in reader:\n",
    "      print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
