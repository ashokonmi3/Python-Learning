{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: Creating New Features\n",
    "\n",
    "## What is Feature Engineering?\n",
    "\n",
    "Feature engineering is the process of transforming raw data into meaningful features that help improve the performance of machine learning models. It is like taking information from your data and making it easier for a model to understand and make predictions. \n",
    "\n",
    "Imagine you're trying to predict the price of a house. You have a lot of raw information like the size of the house, number of rooms, and the year it was built. But sometimes, just using this information directly may not give the best results. Feature engineering helps us to create new pieces of information (features) from the existing data that might be more useful for making predictions.\n",
    "\n",
    "## Why is Feature Engineering Important?\n",
    "\n",
    "- **Improves Model Performance**: By creating better features, we give the model more useful information, which helps it to make better predictions.\n",
    "- **Simplifies the Problem**: It can make the data easier to interpret, so the model can focus on the most important patterns.\n",
    "- **Faster Training**: Sometimes, by creating new features, we can make the model train faster and perform better.\n",
    "\n",
    "## Example of Feature Engineering\n",
    "\n",
    "Let’s consider a simple dataset for predicting the price of a house:\n",
    "\n",
    "| House Size (sq ft) | Number of Rooms | Year Built |\n",
    "|-------------------|----------------|------------|\n",
    "| 1000              | 3              | 2000       |\n",
    "| 1500              | 4              | 2010       |\n",
    "| 800               | 2              | 1995       |\n",
    "| 1200              | 3              | 2015       |\n",
    "\n",
    "Now, suppose we want to predict the price of these houses based on the above features. The model might struggle because the number of rooms and the year built might not directly relate to the price. So, we can create **new features** that might give better insights. Here’s how:\n",
    "\n",
    "### 1. **Age of the House**\n",
    "Instead of using the \"Year Built\" directly, we can create a new feature that shows how old the house is:\n",
    "\n",
    "- **Age of House** = Current Year - Year Built\n",
    "\n",
    "For example, if the current year is 2024:\n",
    "- The house built in 2000 is **24 years old**.\n",
    "- The house built in 2010 is **14 years old**.\n",
    "\n",
    "### 2. **Rooms Per Square Foot**\n",
    "Another useful feature can be the number of rooms relative to the size of the house. This can show how spacious the house feels.\n",
    "\n",
    "- **Rooms per Square Foot** = Number of Rooms / House Size (in sq ft)\n",
    "\n",
    "For example:\n",
    "- House 1: 3 rooms / 1000 sq ft = 0.003 rooms per sq ft\n",
    "- House 2: 4 rooms / 1500 sq ft = 0.0027 rooms per sq ft\n",
    "\n",
    "### Updated Table with New Features\n",
    "\n",
    "| House Size (sq ft) | Number of Rooms | Year Built | Age of House | Rooms per Square Foot |\n",
    "|-------------------|----------------|------------|--------------|-----------------------|\n",
    "| 1000              | 3              | 2000       | 24           | 0.003                 |\n",
    "| 1500              | 4              | 2010       | 14           | 0.0027                |\n",
    "| 800               | 2              | 1995       | 29           | 0.0025                |\n",
    "| 1200              | 3              | 2015       | 9            | 0.0025                |\n",
    "\n",
    "## How Does This Help?\n",
    "\n",
    "- **Age of House**: Older houses might have lower prices, and newer houses might be more expensive. This new feature helps capture that trend.\n",
    "- **Rooms per Square Foot**: This helps the model understand how spacious the house is relative to its size. Larger houses with fewer rooms might be more expensive because they are more spacious.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Feature engineering is about thinking creatively to create new features from existing data that might help your model make better predictions. It requires understanding the problem you're solving and using your judgment to transform the data in a way that adds more value.\n",
    "\n",
    "By creating new features like the age of the house or rooms per square foot, you can give your model better insights to make more accurate predictions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   House Size (sq ft)  Number of Rooms  Year Built\n",
      "0                1000                3        2000\n",
      "1                1500                4        2010\n",
      "2                 800                2        1995\n",
      "3                1200                3        2015\n",
      "\n",
      "DataFrame with New Features:\n",
      "   House Size (sq ft)  Number of Rooms  Year Built  Age of House  \\\n",
      "0                1000                3        2000            24   \n",
      "1                1500                4        2010            14   \n",
      "2                 800                2        1995            29   \n",
      "3                1200                3        2015             9   \n",
      "\n",
      "   Rooms per Square Foot  \n",
      "0               0.003000  \n",
      "1               0.002667  \n",
      "2               0.002500  \n",
      "3               0.002500  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Creating the original dataset\n",
    "data = {\n",
    "    'House Size (sq ft)': [1000, 1500, 800, 1200],\n",
    "    'Number of Rooms': [3, 4, 2, 3],\n",
    "    'Year Built': [2000, 2010, 1995, 2015]\n",
    "}\n",
    "\n",
    "# Convert the dataset into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "# 1. Create 'Age of House' by subtracting 'Year Built' from the current year\n",
    "current_year = datetime.now().year\n",
    "df['Age of House'] = current_year - df['Year Built']\n",
    "\n",
    "# 2. Create 'Rooms per Square Foot' by dividing 'Number of Rooms' by 'House Size (sq ft)'\n",
    "df['Rooms per Square Foot'] = df['Number of Rooms'] / df['House Size (sq ft)']\n",
    "\n",
    "# Show the DataFrame with new features\n",
    "print(\"\\nDataFrame with New Features:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection: Removing Irrelevant Features\n",
    "\n",
    "## What is Feature Selection?\n",
    "\n",
    "**Feature Selection** is the process of choosing the **most important** and **relevant** pieces of information (features) from your dataset while **removing** the **irrelevant**, **redundant**, or **noisy** features.\n",
    "\n",
    "Think of it like cooking a dish: if you have a lot of ingredients, some of them may not help the taste or may even spoil the dish. You only need to keep the **important ingredients** and remove the **unnecessary ones**. In machine learning, these \"important ingredients\" are the **features** that help predict the target you're interested in (like house prices, product sales, etc.).\n",
    "\n",
    "For example, if you're trying to predict the **price of a house**, features like the **size of the house**, **number of rooms**, and **location** are important. However, features like **color of the door** or **name of the street** may not help in predicting the price, so they can be removed.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is Feature Selection Important?\n",
    "\n",
    "1. **Improves Accuracy**: By keeping only the relevant features, your model can make better predictions, leading to more accurate results.\n",
    "2. **Reduces Overfitting**: If you use too many features, your model might get too specific to the training data and fail to work well on new, unseen data. Removing irrelevant features reduces this risk.\n",
    "3. **Speeds up the Model**: Fewer features mean the model can be trained and tested faster.\n",
    "4. **Simplifies the Model**: A simpler model is easier to understand, explain, and interpret, especially for non-technical people.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Perform Feature Selection?\n",
    "\n",
    "Feature selection can be done using different methods. Here are a few simple techniques to help you understand how this works.\n",
    "\n",
    "### 1. **Removing Irrelevant Features**\n",
    "Some features do not affect the target variable (the thing you're trying to predict). For example, if you are predicting the price of a house, the **color of the door** may not have any impact on the price, so it's irrelevant. In this case, you would **remove** this feature.\n",
    "\n",
    "**Example**: \n",
    "\n",
    "If you have the following dataset:\n",
    "\n",
    "| House Size (sq ft) | Number of Rooms | Year Built | Color of the Door | House Price |\n",
    "|--------------------|-----------------|------------|-------------------|-------------|\n",
    "| 1000               | 3               | 2000       | Red               | $300,000    |\n",
    "| 1500               | 4               | 2010       | Blue              | $400,000    |\n",
    "| 800                | 2               | 1995       | Green             | $250,000    |\n",
    "| 1200               | 3               | 2015       | Yellow            | $350,000    |\n",
    "\n",
    "Here, **Color of the Door** has nothing to do with the price of the house, so it can be removed.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Removing Redundant Features**\n",
    "Some features are just a duplicate or a version of another feature. For example, if you have both **\"Size in square feet\"** and **\"Size in square meters\"**, they are closely related. Keeping both is redundant (they carry the same information), so you should keep just one.\n",
    "\n",
    "**Example**: \n",
    "\n",
    "If you have both **\"Height\"** and **\"Height in meters\"**, you can remove one of them because they are giving the same information in different units.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Using Domain Knowledge**\n",
    "Domain knowledge refers to your understanding of the problem you're working on. If you know what affects the target variable, you can make better decisions about which features are important.\n",
    "\n",
    "**Example**: In predicting house prices:\n",
    "- **Number of Rooms** and **House Size** are likely important.\n",
    "- **Age of the House** may be important (because older houses might be cheaper).\n",
    "- **Location** might be a key factor.\n",
    "- **Color of the Door** might not be important, so it can be removed.\n",
    "\n",
    "By applying what you know about the problem, you can make better choices about which features to keep.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Checking Feature Importance with Statistical Tests**\n",
    "In some cases, you can apply statistical tests to find out which features have a stronger relationship with the target variable (what you're predicting). If a feature doesn’t affect the target variable, you can safely remove it.\n",
    "\n",
    "For example:\n",
    "- If you’re predicting **house prices**, you could run tests to see which features like **size**, **number of rooms**, and **year built** are most important in predicting the price.\n",
    "- Features that have **no significant relationship** with the target can be dropped.\n",
    "\n",
    "---\n",
    "\n",
    "## Example to Understand Feature Selection\n",
    "\n",
    "Let’s break it down with an example:\n",
    "\n",
    "We have a dataset of houses, and we want to predict the **price of the house** based on different features:\n",
    "\n",
    "| House Size (sq ft) | Number of Rooms | Year Built | Color of the Door | Distance to Nearest School | House Price |\n",
    "|--------------------|-----------------|------------|-------------------|----------------------------|-------------|\n",
    "| 1000               | 3               | 2000       | Red               | 1 km                       | $300,000    |\n",
    "| 1500               | 4               | 2010       | Blue              | 2 km                       | $400,000    |\n",
    "| 800                | 2               | 1995       | Green             | 0.5 km                     | $250,000    |\n",
    "| 1200               | 3               | 2015       | Yellow            | 1.5 km                     | $350,000    |\n",
    "\n",
    "### Step 1: **Remove Irrelevant Features**\n",
    "- **Color of the Door**: This feature is **irrelevant** to predicting house prices, so it can be removed.\n",
    "\n",
    "### Step 2: **Check for Redundancy**\n",
    "- **House Size** and **Number of Rooms** are related. Generally, larger houses tend to have more rooms. If they provide similar information, we might decide to keep just **House Size** or **Number of Rooms**, depending on which one seems more useful.\n",
    "\n",
    "### Step 3: **Domain Knowledge**\n",
    "- We know that **Location** (e.g., Distance to Nearest School) and **Year Built** could affect the house price. Therefore, we decide to **keep** them.\n",
    "\n",
    "### Step 4: **Final Features**\n",
    "After applying feature selection, our dataset might look like this:\n",
    "\n",
    "| House Size (sq ft) | Number of Rooms | Year Built | Distance to Nearest School | House Price |\n",
    "|--------------------|-----------------|------------|----------------------------|-------------|\n",
    "| 1000               | 3               | 2000       | 1 km                       | $300,000    |\n",
    "| 1500               | 4               | 2010       | 2 km                       | $400,000    |\n",
    "| 800                | 2               | 1995       | 0.5 km                     | $250,000    |\n",
    "| 1200               | 3               | 2015       | 1.5 km                     | $350,000    |\n",
    "\n",
    "Now, we have only the **relevant features** to predict **house price**.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Feature selection is an important step in building a machine learning model because it helps you focus on the **most important information**. By removing irrelevant or redundant features, you can:\n",
    "- Improve the accuracy of your model.\n",
    "- Reduce the time it takes to train the model.\n",
    "- Make the model simpler and easier to understand.\n",
    "\n",
    "By using your **domain knowledge**, **statistical methods**, and logical reasoning, you can carefully choose the best features for your machine learning task!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   House Size (sq ft)  Number of Rooms  Year Built Color of the Door  \\\n",
      "0                1000                3        2000               Red   \n",
      "1                1500                4        2010               Red   \n",
      "2                 800                2        1995               Red   \n",
      "3                1200                3        2015               Red   \n",
      "\n",
      "   Distance to Nearest School  House Price  \n",
      "0                         1.0       300000  \n",
      "1                         2.0       400000  \n",
      "2                         0.5       250000  \n",
      "3                         1.5       350000  \n",
      "\n",
      "DataFrame after Feature Selection (with VarianceThreshold):\n",
      "   House Size (sq ft)  Number of Rooms  Year Built  Distance to Nearest School\n",
      "0              1000.0              3.0      2000.0                         1.0\n",
      "1              1500.0              4.0      2010.0                         2.0\n",
      "2               800.0              2.0      1995.0                         0.5\n",
      "3              1200.0              3.0      2015.0                         1.5\n",
      "\n",
      "House Price (Target):\n",
      "0    300000\n",
      "1    400000\n",
      "2    250000\n",
      "3    350000\n",
      "Name: House Price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample dataset with a feature that has zero variance\n",
    "data = {\n",
    "    'House Size (sq ft)': [1000, 1500, 800, 1200],\n",
    "    'Number of Rooms': [3, 4, 2, 3],\n",
    "    'Year Built': [2000, 2010, 1995, 2015],\n",
    "    # Zero variance: All values are 'Red'\n",
    "    'Color of the Door': ['Red', 'Red', 'Red', 'Red'],\n",
    "    'Distance to Nearest School': [1, 2, 0.5, 1.5],\n",
    "    'House Price': [300000, 400000, 250000, 350000]\n",
    "}\n",
    "\n",
    "# Convert the dataset into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Encoding all categorical features (e.g., 'Color of the Door') using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['Color of the Door'] = label_encoder.fit_transform(df['Color of the Door'])\n",
    "\n",
    "# Feature Selection: Remove features with zero variance\n",
    "# Exclude 'House Price' as it's the target\n",
    "input_features = df.drop(columns=['House Price'])\n",
    "\n",
    "# Apply VarianceThreshold to remove features with zero variance\n",
    "selector = VarianceThreshold(threshold=0)  # Remove features with variance = 0\n",
    "df_selected = selector.fit_transform(input_features)\n",
    "\n",
    "# Convert the result back to a DataFrame, using only the remaining columns\n",
    "df_selected = pd.DataFrame(\n",
    "    df_selected, columns=input_features.columns[selector.get_support()])\n",
    "\n",
    "# Show the DataFrame after removing irrelevant and redundant features\n",
    "print(\"\\nDataFrame after Feature Selection (with VarianceThreshold):\")\n",
    "print(df_selected)\n",
    "\n",
    "# Display the target variable separately (House Price)\n",
    "target = df['House Price']\n",
    "print(\"\\nHouse Price (Target):\")\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
